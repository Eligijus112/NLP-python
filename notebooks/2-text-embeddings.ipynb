{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the needed libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration tracking \n",
    "from tqdm import tqdm \n",
    "\n",
    "# Regular expressions\n",
    "import re\n",
    "\n",
    "# Typehinting \n",
    "from typing import Tuple\n",
    "\n",
    "# Array math \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the text to tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP in Python is fun and very well documented. Let's get started!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(x: str) -> str:\n",
    "    \"\"\"\n",
    "    Function that preprocess the text before tokenization\n",
    "\n",
    "    Args:\n",
    "        x (str): text to preprocess\n",
    "\n",
    "    Returns:\n",
    "        str: preprocessed text\n",
    "    \"\"\" \n",
    "    # Create whitespaces around punctuation\n",
    "    x = re.sub(r'([.,!?;:])', r' \\1 ', x)\n",
    "\n",
    "    # Returns the text \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP in Python is fun and very well documented .  Let's get started ! \n"
     ]
    }
   ],
   "source": [
    "text_preprocessed = preprocess_text(text)\n",
    "print(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_index(x: str) -> Tuple[dict, dict]: \n",
    "    \"\"\"\n",
    "    Function that scans a given text and creates two dictionaries:\n",
    "    - word2idx: dictionary mapping words to integers\n",
    "    - idx2word: dictionary mapping integers to words\n",
    "\n",
    "    Args:\n",
    "        x (str): text to scan\n",
    "\n",
    "    Returns:\n",
    "        Tuple[dict, dict]: word2idx and idx2word dictionaries\n",
    "    \"\"\"\n",
    "    # Spliting the text into words\n",
    "    words = x.split()\n",
    "\n",
    "    # Creating the word2idx dictionary \n",
    "    word2idx = {}\n",
    "    for word in words: \n",
    "        if word not in word2idx: \n",
    "            # The len(word2idx) will always ensure that the \n",
    "            # new index is 1 + the length of the dictionary so far\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "    # Adding the <UNK> token to the dictionary; This token will be used \n",
    "    # on new texts that were not seen during training.\n",
    "    # It will have the last index. \n",
    "    word2idx['<UNK>'] = len(word2idx)\n",
    "\n",
    "    # Reversing the above dictionary and creating the idx2word dictionary\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "    # Returns the dictionaries\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the word2idx and idx2word dictionaries\n",
    "word2idx, idx2word = create_word_index(text_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NLP': 0,\n",
       " 'in': 1,\n",
       " 'Python': 2,\n",
       " 'is': 3,\n",
       " 'fun': 4,\n",
       " 'and': 5,\n",
       " 'very': 6,\n",
       " 'well': 7,\n",
       " 'documented': 8,\n",
       " '.': 9,\n",
       " \"Let's\": 10,\n",
       " 'get': 11,\n",
       " 'started': 12,\n",
       " '!': 13,\n",
       " '<UNK>': 14}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NLP',\n",
       " 1: 'in',\n",
       " 2: 'Python',\n",
       " 3: 'is',\n",
       " 4: 'fun',\n",
       " 5: 'and',\n",
       " 6: 'very',\n",
       " 7: 'well',\n",
       " 8: 'documented',\n",
       " 9: '.',\n",
       " 10: \"Let's\",\n",
       " 11: 'get',\n",
       " 12: 'started',\n",
       " 13: '!',\n",
       " 14: '<UNK>'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that creates the embedding vector \n",
    "def create_embedding_vector(\n",
    "        n_dim: int = 16, \n",
    "        mean: float = 0.0, \n",
    "        variance: float = 1.0,\n",
    "        precision: int = None\n",
    "        ) -> np.array: \n",
    "    \"\"\"\n",
    "    Function that creates a random embedding vector\n",
    "\n",
    "    Args:\n",
    "        n_dim (int, optional): embedding dimension. Defaults to 16.\n",
    "        mean (float, optional): mean of the normal distribution. Defaults to 0.0.\n",
    "        variance (float, optional): variance of the normal distribution. Defaults to 1.0.\n",
    "        precision (int, optional): precision of each of the gotten coordinate. Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        np.array: embedding vector\n",
    "    \"\"\"\n",
    "    # Creating a random normal distribution \n",
    "    X = np.random.normal(mean, variance, (n_dim, ))\n",
    "\n",
    "    # Rounding the coordinates to the given precision\n",
    "    if precision: \n",
    "        X = np.round(X, precision)\n",
    "\n",
    "    # Returns the embedding vector \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the embedding dictionary \n",
    "idx2embeddings = {}\n",
    "\n",
    "# Iterating over the idx2word dictionary\n",
    "for idx in range(len(idx2word)): \n",
    "    # Creating the embedding vector \n",
    "    X = create_embedding_vector(n_dim=6, precision=3)\n",
    "\n",
    "    # Adding the embedding vector to the embedding dictionary\n",
    "    idx2embeddings[idx] = X\n",
    "\n",
    "# Creating the word2embeddings dictionary\n",
    "word2embeddings = {word: idx2embeddings[idx] for word, idx in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: array([ 0.308, -1.003, -0.36 ,  0.57 , -1.106, -0.997]),\n",
       " 1: array([-1.283,  0.709,  0.812,  0.201,  0.339,  1.264]),\n",
       " 2: array([ 1.095, -0.666,  1.32 , -0.668, -0.705,  0.311]),\n",
       " 3: array([ 0.417,  1.088,  1.242,  0.905, -0.061,  1.316]),\n",
       " 4: array([ 1.432, -0.072,  0.622, -0.077,  0.597,  0.722]),\n",
       " 5: array([-0.724, -0.496, -1.652,  1.118, -2.108, -0.996]),\n",
       " 6: array([ 0.733,  0.021,  0.972,  0.363,  0.074, -0.661]),\n",
       " 7: array([-0.284,  1.453,  2.522,  1.027, -1.484,  0.301]),\n",
       " 8: array([ 0.921,  0.19 ,  0.068,  0.517, -0.767,  0.225]),\n",
       " 9: array([-0.317,  0.691, -1.281,  0.624,  2.004,  1.377]),\n",
       " 10: array([ 0.171,  0.607, -1.064, -0.064, -0.091,  0.748]),\n",
       " 11: array([-0.151,  1.137,  0.783, -0.689, -1.473, -0.753]),\n",
       " 12: array([1.699, 0.021, 1.422, 0.316, 0.317, 0.064]),\n",
       " 13: array([-0.804,  0.156, -0.298,  0.15 , -0.686,  0.752]),\n",
       " 14: array([ 0.538,  0.405,  0.501, -0.245, -1.946,  0.282])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NLP': array([ 0.308, -1.003, -0.36 ,  0.57 , -1.106, -0.997]),\n",
       " 'in': array([-1.283,  0.709,  0.812,  0.201,  0.339,  1.264]),\n",
       " 'Python': array([ 1.095, -0.666,  1.32 , -0.668, -0.705,  0.311]),\n",
       " 'is': array([ 0.417,  1.088,  1.242,  0.905, -0.061,  1.316]),\n",
       " 'fun': array([ 1.432, -0.072,  0.622, -0.077,  0.597,  0.722]),\n",
       " 'and': array([-0.724, -0.496, -1.652,  1.118, -2.108, -0.996]),\n",
       " 'very': array([ 0.733,  0.021,  0.972,  0.363,  0.074, -0.661]),\n",
       " 'well': array([-0.284,  1.453,  2.522,  1.027, -1.484,  0.301]),\n",
       " 'documented': array([ 0.921,  0.19 ,  0.068,  0.517, -0.767,  0.225]),\n",
       " '.': array([-0.317,  0.691, -1.281,  0.624,  2.004,  1.377]),\n",
       " \"Let's\": array([ 0.171,  0.607, -1.064, -0.064, -0.091,  0.748]),\n",
       " 'get': array([-0.151,  1.137,  0.783, -0.689, -1.473, -0.753]),\n",
       " 'started': array([1.699, 0.021, 1.422, 0.316, 0.317, 0.064]),\n",
       " '!': array([-0.804,  0.156, -0.298,  0.15 , -0.686,  0.752]),\n",
       " '<UNK>': array([ 0.538,  0.405,  0.501, -0.245, -1.946,  0.282])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.308 -1.003 -0.36   0.57  -1.106 -0.997]\n",
      " [-1.283  0.709  0.812  0.201  0.339  1.264]\n",
      " [ 1.095 -0.666  1.32  -0.668 -0.705  0.311]\n",
      " [ 0.417  1.088  1.242  0.905 -0.061  1.316]\n",
      " [ 1.432 -0.072  0.622 -0.077  0.597  0.722]\n",
      " [-0.724 -0.496 -1.652  1.118 -2.108 -0.996]\n",
      " [ 0.733  0.021  0.972  0.363  0.074 -0.661]\n",
      " [-0.284  1.453  2.522  1.027 -1.484  0.301]\n",
      " [ 0.921  0.19   0.068  0.517 -0.767  0.225]\n",
      " [-0.317  0.691 -1.281  0.624  2.004  1.377]\n",
      " [ 0.171  0.607 -1.064 -0.064 -0.091  0.748]\n",
      " [-0.151  1.137  0.783 -0.689 -1.473 -0.753]\n",
      " [ 1.699  0.021  1.422  0.316  0.317  0.064]\n",
      " [-0.804  0.156 -0.298  0.15  -0.686  0.752]\n",
      " [ 0.538  0.405  0.501 -0.245 -1.946  0.282]]\n",
      "(15, 6)\n"
     ]
    }
   ],
   "source": [
    "# Creating the embedding matrix\n",
    "embedding_matrix = np.array([idx2embeddings[idx] for idx in idx2embeddings])\n",
    "\n",
    "# Printing the matrix\n",
    "print(embedding_matrix)\n",
    "\n",
    "# Printing the shape\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
