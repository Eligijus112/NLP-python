{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed packages\n",
    "import torch \n",
    "from torch import nn\n",
    "import pandas as pd \n",
    "import re \n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "# Spliting the data into train and test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data \n",
    "d = pd.read_csv('input/Tweets.csv', header=None)\n",
    "\n",
    "# Adding the columns \n",
    "d.columns = ['INDEX', 'GAME', \"SENTIMENT\", 'TEXT']\n",
    "\n",
    "# Leaving only the positive and the negative sentiments \n",
    "d = d[d['SENTIMENT'].isin(['Positive', 'Negative'])]\n",
    "\n",
    "# Encoding the sentiments that the negative will be 1 and the positive 0\n",
    "d['SENTIMENT'] = d['SENTIMENT'].apply(lambda x: 0 if x == 'Positive' else 1)\n",
    "\n",
    "# Dropping missing values\n",
    "d = d.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.sample(10)[['SENTIMENT', 'TEXT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting to train test \n",
    "train, test = train_test_split(d, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reseting the indexes \n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f'Train shape: {train.shape}')\n",
    "print(f'Test shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_index(x: str, shift_for_padding: bool = False, char_level: bool = False) -> Tuple[dict, dict]: \n",
    "    \"\"\"\n",
    "    Function that scans a given text and creates two dictionaries:\n",
    "    - word2idx: dictionary mapping words to integers\n",
    "    - idx2word: dictionary mapping integers to words\n",
    "\n",
    "    Args:\n",
    "        x (str): text to scan\n",
    "        shift_for_padding (bool, optional): If True, the function will add 1 to all the indexes.\n",
    "            This is done to reserve the 0 index for padding. Defaults to False.\n",
    "        char_level (bool, optional): If True, the function will create a character level dictionary.\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[dict, dict]: word2idx and idx2word dictionaries\n",
    "    \"\"\"\n",
    "    # Ensuring that the text is a string\n",
    "    if not isinstance(x, str):\n",
    "        try: \n",
    "            x = str(x)\n",
    "        except:\n",
    "            raise Exception('The text must be a string or a string convertible object')\n",
    "        \n",
    "    # Spliting the text into words\n",
    "    words = []\n",
    "    if char_level:\n",
    "        # The list() function of a string will return a list of characters\n",
    "        words = list(x)\n",
    "    else:\n",
    "        # Spliting the text into words by spaces\n",
    "        words = x.split(' ')\n",
    "\n",
    "    # Creating the word2idx dictionary \n",
    "    word2idx = {}\n",
    "    for word in words: \n",
    "        if word not in word2idx: \n",
    "            # The len(word2idx) will always ensure that the \n",
    "            # new index is 1 + the length of the dictionary so far\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "    # Adding the <UNK> token to the dictionary; This token will be used \n",
    "    # on new texts that were not seen during training.\n",
    "    # It will have the last index. \n",
    "    word2idx['<UNK>'] = len(word2idx)\n",
    "\n",
    "    if shift_for_padding:\n",
    "        # Adding 1 to all the indexes; \n",
    "        # The 0 index will be reserved for padding\n",
    "        word2idx = {k: v + 1 for k, v in word2idx.items()}\n",
    "\n",
    "    # Reversing the above dictionary and creating the idx2word dictionary\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "    # Returns the dictionaries\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all the texts into one string\n",
    "text = ' '.join(train['TEXT'].values)\n",
    "\n",
    "# Creating the word2idx and idx2word dictionaries\n",
    "word2idx, idx2word = create_word_index(text, shift_for_padding=True, char_level=True)\n",
    "\n",
    "# Printing the size of the vocabulary\n",
    "print(f'The size of the vocabulary is: {len(word2idx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the top 10 words\n",
    "print(f'The top 10 words are: {list(word2idx.keys())[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row in the train and test set, we will create a list of integers\n",
    "# that will represent the words in the text\n",
    "train['text_int'] = train['TEXT'].apply(lambda x: [word2idx.get(word, word2idx['<UNK>']) for word in list(x)])\n",
    "test['text_int'] = test['TEXT'].apply(lambda x: [word2idx.get(word, word2idx['<UNK>']) for word in list(x)])\n",
    "\n",
    "# Calculating the length of sequences in the train set \n",
    "train['seq_len'] = train['text_int'].apply(lambda x: len(x))\n",
    "\n",
    "# Describing the length of the sequences\n",
    "train['seq_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(x: list, pad_length: int) -> list:\n",
    "    \"\"\"\n",
    "    Function that pads a given list of integers to a given length\n",
    "\n",
    "    Args:\n",
    "        x (list): list of integers to pad\n",
    "        pad_length (int): length to pad\n",
    "\n",
    "    Returns:\n",
    "        list: padded list of integers\n",
    "    \"\"\"\n",
    "    # Getting the length of the list\n",
    "    len_x = len(x)\n",
    "\n",
    "    # Checking if the length of the list is less than the pad_length\n",
    "    if len_x < pad_length: \n",
    "        # Padding the list with 0s\n",
    "        x = x + [0] * (pad_length - len_x)\n",
    "    else: \n",
    "        # Truncating the list to the desired length\n",
    "        x = x[:pad_length]\n",
    "\n",
    "    # Returning the padded list\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the train and test sequences \n",
    "train['text_int'] = train['text_int'].apply(lambda x: pad_sequences(x, 200))\n",
    "test['text_int'] = test['text_int'].apply(lambda x: pad_sequences(x, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the activation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: float) -> float: \n",
    "    \"\"\"\n",
    "    Function that calculates the sigmoid of a given value\n",
    "\n",
    "    Args:\n",
    "        x (float): value to calculate the sigmoid\n",
    "\n",
    "    Returns:\n",
    "        float: sigmoid of the given value in (0, 1)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x: float) -> float: \n",
    "    \"\"\"\n",
    "    Function that calculates the tanh of a given value\n",
    "\n",
    "    Args:\n",
    "        x (float): value to calculate the tanh\n",
    "\n",
    "    Returns:\n",
    "        float: tanh of the given value in (-1, 1)\n",
    "    \"\"\"\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the forget gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForgetGate: \n",
    "    \"\"\"\n",
    "    Class that implements the forget gate of an LSTM cell\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            w1: float = np.random.normal(), \n",
    "            w2: float = np.random.normal(),\n",
    "            b1: float = np.random.normal(),\n",
    "            long_term_memory: float = np.random.normal(), \n",
    "            short_term_memory: float = np.random.normal(), \n",
    "            ):\n",
    "        \"\"\"\n",
    "        Constructor of the class\n",
    "\n",
    "        Args:\n",
    "            long_term_memory (float): long term memory\n",
    "            short_term_memory (float): short term memory\n",
    "            w1 (float): weight 1\n",
    "            w2 (float): weight 2\n",
    "            b1 (float): bias term 1\n",
    "        \"\"\"\n",
    "        # Saving the input\n",
    "        self.long_term_memory = long_term_memory\n",
    "        self.short_term_memory = short_term_memory\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.b1 = b1\n",
    "\n",
    "    def forward(self, x: float) -> float: \n",
    "        \"\"\"\n",
    "        Function that calculates the output of the forget gate\n",
    "\n",
    "        Args:\n",
    "            x (float): input to the forget gate\n",
    "\n",
    "        Returns:\n",
    "            float: output of the forget gate\n",
    "        \"\"\"\n",
    "        # Calculates the percentage of the long term memory that will be kept\n",
    "        percentage_to_keep = sigmoid((self.w1 * x  + self.w2 * self.short_term_memory) + self.b1)\n",
    "\n",
    "        # Updating the long term memory\n",
    "        self.long_term_memory = self.long_term_memory * percentage_to_keep\n",
    "\n",
    "        # The output of the forget gate is the new long term memory and the short term memory\n",
    "        return self.long_term_memory, self.short_term_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating \n",
    "forget_gate = ForgetGate()\n",
    "\n",
    "print(f'Initial long term memory: {forget_gate.long_term_memory}')\n",
    "print(f'Initial short term memory: {forget_gate.short_term_memory}')\n",
    "\n",
    "# Calculating the output of the forget gate\n",
    "lt, st = forget_gate.forward(0.5)\n",
    "\n",
    "print(f'Long term memory: {lt}')\n",
    "print(f'Short term memory: {st}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the input gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputGate:\n",
    "    def __init__(\n",
    "            self, \n",
    "            w3: float = np.random.normal(), \n",
    "            w4: float = np.random.normal(),\n",
    "            w5: float = np.random.normal(),\n",
    "            w6: float = np.random.normal(),\n",
    "            b2: float = np.random.normal(),\n",
    "            b3: float = np.random.normal(),\n",
    "            long_term_memory: float = np.random.normal(), \n",
    "            short_term_memory: float = np.random.normal(), \n",
    "            ):\n",
    "        \"\"\"\n",
    "        Constructor of the class\n",
    "\n",
    "        Args:\n",
    "            long_term_memory (float): long term memory\n",
    "            short_term_memory (float): short term memory\n",
    "            w3 (float): weight 3\n",
    "            w4 (float): weight 4\n",
    "            w5 (float): weight 5\n",
    "            w6 (float): weight 6\n",
    "            b2 (float): bias 2\n",
    "            b3 (float): bias 3\n",
    "        \"\"\"\n",
    "        # Saving the input\n",
    "        self.long_term_memory = long_term_memory\n",
    "        self.short_term_memory = short_term_memory\n",
    "        self.w3 = w3\n",
    "        self.w4 = w4\n",
    "        self.w5 = w5\n",
    "        self.w6 = w6\n",
    "        self.b2 = b2\n",
    "        self.b3 = b3\n",
    "\n",
    "    def forward(self, x: float) -> float:\n",
    "        \"\"\"\n",
    "        Function that calculates the output of the input gate\n",
    "\n",
    "        Args:\n",
    "            x (float): input to the input gate\n",
    "\n",
    "        Returns:\n",
    "            float: output of the input gate\n",
    "        \"\"\"\n",
    "        # Calculating the memory signal \n",
    "        memory_signal = tanh((self.w3 * x + self.w4 * self.short_term_memory) + self.b2)\n",
    "\n",
    "        # Calculating the percentage of memory to keep \n",
    "        percentage_to_keep = sigmoid((self.w5 * x + self.w6 * self.short_term_memory) + self.b3) \n",
    "\n",
    "        # Multiplying the memory signal by the percentage to keep\n",
    "        memory_signal = memory_signal * percentage_to_keep\n",
    "\n",
    "        # Updating the long term memory\n",
    "        self.long_term_memory = self.long_term_memory + memory_signal\n",
    "\n",
    "        # The output of the input gate is the new long term memory and the short term memory\n",
    "        return self.long_term_memory, self.short_term_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the input gate object \n",
    "input_gate = InputGate(long_term_memory=lt, short_term_memory=st)\n",
    "\n",
    "# Forward propagating \n",
    "lt, st = input_gate.forward(0.5)\n",
    "\n",
    "print(f'Long term memory: {lt}')\n",
    "print(f'Short term memory: {st}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the output gate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputGate:\n",
    "    def __init__(\n",
    "            self, \n",
    "            w7: float = np.random.normal(), \n",
    "            w8: float = np.random.normal(),\n",
    "            b4: float = np.random.normal(),\n",
    "            long_term_memory: float = np.random.normal(), \n",
    "            short_term_memory: float = np.random.normal(), \n",
    "            ):\n",
    "        \"\"\"\n",
    "        Constructor of the class\n",
    "\n",
    "        Args:\n",
    "            long_term_memory (float): long term memory\n",
    "            short_term_memory (float): short term memory\n",
    "            w7 (float): weight 7\n",
    "            w8 (float): weight 8\n",
    "            w9 (float): weight 9\n",
    "            w10 (float): weight 10\n",
    "            b4 (float): bias 4\n",
    "            b5 (float): bias 5\n",
    "        \"\"\"\n",
    "        # Saving the input\n",
    "        self.long_term_memory = long_term_memory\n",
    "        self.short_term_memory = short_term_memory\n",
    "        self.w7 = w7\n",
    "        self.w8 = w8\n",
    "        self.b4 = b4\n",
    "\n",
    "    def forward(self, x: float) -> float:\n",
    "        \"\"\"\n",
    "        Function that calculates the output of the output gate\n",
    "\n",
    "        Args:\n",
    "            x (float): input to the output gate\n",
    "\n",
    "        Returns:\n",
    "            float: output of the output gate\n",
    "        \"\"\"\n",
    "        # Calculating the short term memory signal \n",
    "        short_term_memory_signal = tanh(self.long_term_memory)\n",
    "\n",
    "        # Calculating the percentage of short term memory to keep \n",
    "        percentage_to_keep = sigmoid((self.w7 * x + self.w8 * self.short_term_memory) + self.b4) \n",
    "\n",
    "        # Multiplying the short term memory signal by the percentage to keep\n",
    "        short_term_memory_signal = short_term_memory_signal * percentage_to_keep\n",
    "\n",
    "        # Updating the short term memory\n",
    "        self.short_term_memory = short_term_memory_signal\n",
    "\n",
    "        # The output of the output gate is the new long term memory and the short term memory\n",
    "        return self.long_term_memory, self.short_term_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the output gate object\n",
    "output_gate = OutputGate(long_term_memory=lt, short_term_memory=st)\n",
    "\n",
    "# Forward propagating\n",
    "lt, st = output_gate.forward(0.5)\n",
    "\n",
    "print(f'Long term memory: {lt}')\n",
    "print(f'Short term memory: {st}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a simple LSTM class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefining the forget, input and output gates as functions \n",
    "def forget_gate(x: float, w1: float, w2: float, b1: float, long_term_memory: float, short_term_memory: float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Function that calculates the output of the forget gate\n",
    "\n",
    "    Args:\n",
    "        x (float): input to the forget gate\n",
    "        w1 (float): weight 1\n",
    "        w2 (float): weight 2\n",
    "        b1 (float): bias 1\n",
    "        long_term_memory (float): long term memory\n",
    "        short_term_memory (float): short term memory\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: output of the forget gate\n",
    "    \"\"\"\n",
    "    # Calculates the percentage of the long term memory that will be kept\n",
    "    percentage_to_keep = sigmoid((w1 * x  + w2 * short_term_memory) + b1)\n",
    "\n",
    "    # Updating the long term memory\n",
    "    long_term_memory = long_term_memory * percentage_to_keep\n",
    "\n",
    "    # The output of the forget gate is the new long term memory and the short term memory\n",
    "    return long_term_memory, short_term_memory\n",
    "\n",
    "def input_gate(x: float, w3: float, w4: float, w5: float, w6: float, b2: float, b3: float, long_term_memory: float, short_term_memory: float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Function that calculates the output of the input gate\n",
    "\n",
    "    Args:\n",
    "        x (float): input to the input gate\n",
    "        w3 (float): weight 3\n",
    "        w4 (float): weight 4\n",
    "        w5 (float): weight 5\n",
    "        w6 (float): weight 6\n",
    "        b2 (float): bias 2\n",
    "        b3 (float): bias 3\n",
    "        long_term_memory (float): long term memory\n",
    "        short_term_memory (float): short term memory\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: output of the input gate\n",
    "    \"\"\"\n",
    "    # Calculating the memory signal \n",
    "    memory_signal = tanh((w3 * x + w4 * short_term_memory) + b2)\n",
    "\n",
    "    # Calculating the percentage of memory to keep \n",
    "    percentage_to_keep = sigmoid((w5 * x + w6 * short_term_memory) + b3) \n",
    "\n",
    "    # Multiplying the memory signal by the percentage to keep\n",
    "    memory_signal = memory_signal * percentage_to_keep\n",
    "\n",
    "    # Updating the long term memory\n",
    "    long_term_memory = long_term_memory + memory_signal\n",
    "\n",
    "    # The output of the input gate is the new long term memory and the short term memory\n",
    "    return long_term_memory, short_term_memory\n",
    "\n",
    "def output_gate(x: float, w7: float, w8: float, b4: float, long_term_memory: float, short_term_memory: float) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Function that calculates the output of the output gate\n",
    "\n",
    "    Args:\n",
    "        x (float): input to the output gate\n",
    "        w7 (float): weight 7\n",
    "        w8 (float): weight 8\n",
    "        b4 (float): bias 4\n",
    "        long_term_memory (float): long term memory\n",
    "        short_term_memory (float): short term memory\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: output of the output gate\n",
    "    \"\"\"\n",
    "    # Calculating the short term memory signal \n",
    "    short_term_memory_signal = tanh(long_term_memory)\n",
    "\n",
    "    # Calculating the percentage of short term memory to keep \n",
    "    percentage_to_keep = sigmoid((w7 * x + w8 * short_term_memory) + b4) \n",
    "\n",
    "    # Multiplying the short term memory signal by the percentage to keep\n",
    "    short_term_memory_signal = short_term_memory_signal * percentage_to_keep\n",
    "\n",
    "    # Updating the short term memory\n",
    "    short_term_memory = short_term_memory_signal\n",
    "\n",
    "    # The output of the output gate is the new long term memory and the short term memory\n",
    "    return long_term_memory, short_term_memory \n",
    "\n",
    "\n",
    "class simpleLSTM: \n",
    "    def __init__(\n",
    "            self, \n",
    "            w1: float = np.random.normal(),\n",
    "            w2: float = np.random.normal(),\n",
    "            w3: float = np.random.normal(),\n",
    "            w4: float = np.random.normal(),\n",
    "            w5: float = np.random.normal(),\n",
    "            w6: float = np.random.normal(),\n",
    "            w7: float = np.random.normal(),\n",
    "            w8: float = np.random.normal(),\n",
    "            b1: float = np.random.normal(),\n",
    "            b2: float = np.random.normal(),\n",
    "            b3: float = np.random.normal(),\n",
    "            b4: float = np.random.normal(),\n",
    "            long_term_memory: float = np.random.normal(),\n",
    "            short_term_memory: float = np.random.normal(),\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Constructor of the class\n",
    "\n",
    "        Args:\n",
    "            long_term_memory (float): long term memory\n",
    "            short_term_memory (float): short term memory\n",
    "            w1 (float): weight 1\n",
    "            w2 (float): weight 2\n",
    "            w3 (float): weight 3\n",
    "            w4 (float): weight 4\n",
    "            w5 (float): weight 5\n",
    "            w6 (float): weight 6\n",
    "            w7 (float): weight 7\n",
    "            w8 (float): weight 8\n",
    "            b1 (float): bias 1\n",
    "            b2 (float): bias 2\n",
    "            b3 (float): bias 3\n",
    "            b4 (float): bias 4\n",
    "        \"\"\"\n",
    "\n",
    "        # Saving the input\n",
    "        self.long_term_memory = long_term_memory\n",
    "        self.short_term_memory = short_term_memory\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "        self.w3 = w3\n",
    "        self.w4 = w4\n",
    "        self.w5 = w5\n",
    "        self.w6 = w6\n",
    "        self.w7 = w7\n",
    "        self.w8 = w8\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "        self.b3 = b3\n",
    "        self.b4 = b4\n",
    "\n",
    "    def forward(self, x: float) -> float:\n",
    "        \"\"\"\n",
    "        Function that calculates the output of the simple LSTM cell\n",
    "\n",
    "        Args:\n",
    "            x (float): input to the simple LSTM cell\n",
    "\n",
    "        Returns:\n",
    "            float: output of the simple LSTM cell\n",
    "        \"\"\"\n",
    "        # Calculating the output of the forget gate\n",
    "        lt, st = forget_gate(x, self.w1, self.w2, self.b1, self.long_term_memory, self.short_term_memory)\n",
    "\n",
    "        # Updating the long term memory\n",
    "        self.long_term_memory = lt\n",
    "\n",
    "        # Calculating the output of the input gate\n",
    "        lt, st = input_gate(x, self.w3, self.w4, self.w5, self.w6, self.b2, self.b3, self.long_term_memory, self.short_term_memory)\n",
    "\n",
    "        # Updating the long term memory\n",
    "        self.long_term_memory = lt\n",
    "\n",
    "        # Calculating the output of the output gate\n",
    "        lt, st = output_gate(x, self.w7, self.w8, self.b4, self.long_term_memory, self.short_term_memory)\n",
    "\n",
    "        # Updating the short term memory\n",
    "        self.short_term_memory = st\n",
    "\n",
    "        # The output of the simple LSTM cell is the new long term memory and the short term memory\n",
    "        return self.long_term_memory, self.short_term_memory\n",
    "        \n",
    "    def forward_sequence(self, x: list) -> list:\n",
    "        \"\"\"\n",
    "        Function that forward propagates a sequence of inputs through the simple LSTM cell\n",
    "\n",
    "        Args:\n",
    "            x (list): sequence of inputs to the simple LSTM cell\n",
    "\n",
    "        Returns:\n",
    "            list: sequence of outputs of the simple LSTM cell\n",
    "        \"\"\"\n",
    "        # Creating a list to store the outputs\n",
    "        outputs = []\n",
    "\n",
    "        # Forward propagating each input\n",
    "        for input in x: \n",
    "            # Forward propagating the input\n",
    "            _, st = self.forward(input)\n",
    "\n",
    "            # Appending the output to the list\n",
    "            outputs.append(st)\n",
    "\n",
    "        # Returning the list of outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the simple LSTM cell object\n",
    "simple_lstm = simpleLSTM()\n",
    "\n",
    "# Creating a sequence of x\n",
    "x = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Forward propagating the sequence\n",
    "outputs = simple_lstm.forward_sequence(x)\n",
    "\n",
    "# Rounding \n",
    "outputs = [round(output, 2) for output in outputs]\n",
    "\n",
    "# Printing the outputs\n",
    "print(f'The outputs of the simple LSTM cell are: {outputs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the torch model for sentiment classification \n",
    "class SentimentClassifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class that defines the sentiment classifier model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=1, batch_first=True)\n",
    "        self.fc = nn.Linear(1, 1)  # Output with a single neuron for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Embedding layer\n",
    "        output, _ = self.lstm(x)  # RNN layer\n",
    "\n",
    "        # Use the short term memory from the last time step as the representation of the sequence\n",
    "        x = output[:, -1, :]\n",
    "\n",
    "        # Fully connected layer with a single neuron\n",
    "        x = self.fc(x) \n",
    "        \n",
    "        # Converting to probabilities\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        # Flattening the output\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initiating the model \n",
    "model = SentimentClassifier(vocab_size=len(word2idx), embedding_dim=16)\n",
    "\n",
    "# Initiating the criterion and the optimizer\n",
    "criterion = nn.BCELoss() # Binary cross entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the data loader \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The x is named as text_int and the y as airline_sentiment\n",
    "        x = self.data.iloc[idx]['text_int']\n",
    "        y = self.data.iloc[idx]['SENTIMENT']\n",
    "        \n",
    "        # Converting the x and y to torch tensors\n",
    "        x = torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "\n",
    "        # Converting the y variable to float \n",
    "        y = y.float()\n",
    "\n",
    "        # Returning the x and y\n",
    "        return x, y\n",
    "    \n",
    "# Creating the train and test loaders\n",
    "train_loader = DataLoader(TextClassificationDataset(train), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TextClassificationDataset(test), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Setting the model to train mode\n",
    "model.train()\n",
    "\n",
    "# Saving of the loss values\n",
    "losses = []\n",
    "\n",
    "# Iterating through epochs\n",
    "for epoch in range(epochs):\n",
    "    # Initiating the total loss \n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the model's parameters\n",
    "\n",
    "        # Adding the loss to the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculating the average loss\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Appending the loss to the list containing the losses\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    # Printing the loss every n epochs\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the loss \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model to eval model\n",
    "model.eval()\n",
    "\n",
    "# List to track the test acc \n",
    "total_correct = 0\n",
    "total_obs = 0\n",
    "\n",
    "# Iterating over the test set\n",
    "for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "    outputs = model(inputs)  # Forward pass\n",
    "\n",
    "    # Getting the number of correct predictions \n",
    "    correct = ((outputs > 0.5).float() == labels).float().sum()\n",
    "\n",
    "    # Getting the total number of predictions\n",
    "    total = labels.size(0)\n",
    "\n",
    "    # Updating the total correct and total observations\n",
    "    total_correct += correct\n",
    "    total_obs += total\n",
    "\n",
    "print(f'The test accuracy is: {total_correct / total_obs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
