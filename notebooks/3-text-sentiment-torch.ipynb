{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed packages\n",
    "import torch \n",
    "from torch import nn\n",
    "import pandas as pd \n",
    "import re \n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "# Spliting the data into train and test\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data \n",
    "d = pd.read_csv('input/Tweets.csv', header=None)\n",
    "\n",
    "# Adding the columns \n",
    "d.columns = ['INDEX', 'GAME', \"SENTIMENT\", 'TEXT']\n",
    "\n",
    "# Leaving only the positive and the negative sentiments \n",
    "d = d[d['SENTIMENT'].isin(['Positive', 'Negative'])]\n",
    "\n",
    "# Encoding the sentiments that the negative will be 1 and the positive 0\n",
    "d['SENTIMENT'] = d['SENTIMENT'].apply(lambda x: 0 if x == 'Positive' else 1)\n",
    "\n",
    "# Dropping missing values\n",
    "d = d.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(x: str) -> str:\n",
    "    \"\"\"\n",
    "    Function that preprocess the text before tokenization\n",
    "\n",
    "    Args:\n",
    "        x (str): text to preprocess\n",
    "\n",
    "    Returns:\n",
    "        str: preprocessed text\n",
    "    \"\"\" \n",
    "    # Create whitespaces around punctuation\n",
    "    x = re.sub(r'([.,!?;:])', r' \\1 ', x)\n",
    "\n",
    "    # Returns the text \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the text\n",
    "d['TEXT'] = d['TEXT'].apply(preprocess_text)\n",
    "\n",
    "# Spliting to train test \n",
    "train, test = train_test_split(d, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reseting the indexes \n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f'Train shape: {train.shape}')\n",
    "print(f'Test shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_index(x: str, shift_for_padding: bool = False) -> Tuple[dict, dict]: \n",
    "    \"\"\"\n",
    "    Function that scans a given text and creates two dictionaries:\n",
    "    - word2idx: dictionary mapping words to integers\n",
    "    - idx2word: dictionary mapping integers to words\n",
    "\n",
    "    Args:\n",
    "        x (str): text to scan\n",
    "        shift_for_padding (bool, optional): If True, the function will add 1 to all the indexes.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[dict, dict]: word2idx and idx2word dictionaries\n",
    "    \"\"\"\n",
    "    # Spliting the text into words\n",
    "    words = x.split()\n",
    "\n",
    "    # Creating the word2idx dictionary \n",
    "    word2idx = {}\n",
    "    for word in words: \n",
    "        if word not in word2idx: \n",
    "            # The len(word2idx) will always ensure that the \n",
    "            # new index is 1 + the length of the dictionary so far\n",
    "            word2idx[word] = len(word2idx)\n",
    "\n",
    "    # Adding the <UNK> token to the dictionary; This token will be used \n",
    "    # on new texts that were not seen during training.\n",
    "    # It will have the last index. \n",
    "    word2idx['<UNK>'] = len(word2idx)\n",
    "\n",
    "    if shift_for_padding:\n",
    "        # Adding 1 to all the indexes; \n",
    "        # The 0 index will be reserved for padding\n",
    "        word2idx = {k: v + 1 for k, v in word2idx.items()}\n",
    "\n",
    "    # Reversing the above dictionary and creating the idx2word dictionary\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "    # Returns the dictionaries\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining all the texts into one string\n",
    "text = ' '.join(train['TEXT'].values)\n",
    "\n",
    "# Creating the word2idx and idx2word dictionaries\n",
    "word2idx, idx2word = create_word_index(text, shift_for_padding=True)\n",
    "\n",
    "# Printing the size of the vocabulary\n",
    "print(f'The size of the vocabulary is: {len(word2idx)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each row in the train and test set, we will create a list of integers\n",
    "# that will represent the words in the text\n",
    "train['text_int'] = train['TEXT'].apply(lambda x: [word2idx.get(word, word2idx['<UNK>']) for word in x.split()])\n",
    "test['text_int'] = test['TEXT'].apply(lambda x: [word2idx.get(word, word2idx['<UNK>']) for word in x.split()])\n",
    "\n",
    "# Calculating the length of sequences in the train set \n",
    "train['seq_len'] = train['text_int'].apply(lambda x: len(x))\n",
    "\n",
    "# Describing the length of the sequences\n",
    "train['seq_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(x: list, pad_length: int) -> list:\n",
    "    \"\"\"\n",
    "    Function that pads a given list of integers to a given length\n",
    "\n",
    "    Args:\n",
    "        x (list): list of integers to pad\n",
    "        pad_length (int): length to pad\n",
    "\n",
    "    Returns:\n",
    "        list: padded list of integers\n",
    "    \"\"\"\n",
    "    # Getting the length of the list\n",
    "    len_x = len(x)\n",
    "\n",
    "    # Checking if the length of the list is less than the pad_length\n",
    "    if len_x < pad_length: \n",
    "        # Padding the list with 0s\n",
    "        x = x + [0] * (pad_length - len_x)\n",
    "    else: \n",
    "        # Truncating the list to the desired length\n",
    "        x = x[:pad_length]\n",
    "\n",
    "    # Returning the padded list\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the train and test sequences \n",
    "train['text_int'] = train['text_int'].apply(lambda x: pad_sequences(x, 30))\n",
    "test['text_int'] = test['text_int'].apply(lambda x: pad_sequences(x, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the torch model for sentiment classification \n",
    "class SentimentClassifier(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Class that defines the sentiment classifier model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim)\n",
    "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=1, batch_first=True)\n",
    "        self.fc = nn.Linear(1, 1)  # Output with a single neuron for binary classification\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Embedding layer\n",
    "        output, _ = self.rnn(x)  # RNN layer\n",
    "        # Use the hidden state from the last time step as the representation of the sequence\n",
    "        x = output[:, -1, :]\n",
    "\n",
    "        # Fully connected layer with a single neuron\n",
    "        x = self.fc(x) \n",
    "        \n",
    "        # Converting to probabilities\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        # Flattening the output\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initiating the model \n",
    "model = SentimentClassifier(vocab_size=len(word2idx), embedding_dim=16)\n",
    "\n",
    "# Initiating the criterion and the optimizer\n",
    "criterion = nn.BCELoss() # Binary cross entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the data loader \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The x is named as text_int and the y as airline_sentiment\n",
    "        x = self.data.iloc[idx]['text_int']\n",
    "        y = self.data.iloc[idx]['SENTIMENT']\n",
    "        \n",
    "        # Converting the x and y to torch tensors\n",
    "        x = torch.tensor(x)\n",
    "        y = torch.tensor(y)\n",
    "\n",
    "        # Converting the y variable to float \n",
    "        y = y.float()\n",
    "\n",
    "        # Returning the x and y\n",
    "        return x, y\n",
    "    \n",
    "# Creating the train and test loaders\n",
    "train_loader = DataLoader(TextClassificationDataset(train), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(TextClassificationDataset(test), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of epochs\n",
    "epochs = 100\n",
    "\n",
    "# Setting the model to train mode\n",
    "model.train()\n",
    "\n",
    "# Saving of the loss values\n",
    "losses = []\n",
    "\n",
    "# Iterating through epochs\n",
    "for epoch in range(epochs):\n",
    "    # Initiating the total loss \n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update the model's parameters\n",
    "\n",
    "        # Adding the loss to the total loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculating the average loss\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Appending the loss to the list containing the losses\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "    # Printing the loss every n epochs\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the loss \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss vs. Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model to eval model\n",
    "model.eval()\n",
    "\n",
    "# List to track the test acc \n",
    "total_correct = 0\n",
    "total_obs = 0\n",
    "\n",
    "# Iterating over the test set\n",
    "for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "    outputs = model(inputs)  # Forward pass\n",
    "\n",
    "    # Getting the number of correct predictions \n",
    "    correct = ((outputs > 0.5).float() == labels).float().sum()\n",
    "\n",
    "    # Getting the total number of predictions\n",
    "    total = labels.size(0)\n",
    "\n",
    "    # Updating the total correct and total observations\n",
    "    total_correct += correct\n",
    "    total_obs += total\n",
    "\n",
    "print(f'The test accuracy is: {total_correct / total_obs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
